# Fine-Tuning LLM Models: GPT-3.5 —Ç–∞ DistilBERT

–ü—Ä–æ–µ–∫—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä—É—î –¥–≤–∞ –ø—ñ–¥—Ö–æ–¥–∏ –¥–æ fine-tuning –º–æ–¥–µ–ª–µ–π –≤–µ–ª–∏–∫–∏—Ö –º–æ–≤–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π (LLM):
1. **Fine-tuning GPT-3.5-turbo** –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —á–∞—Ç-–±–æ—Ç–∞ –≤ —Å—Ç–∏–ª—ñ –ö—É–∑—å–º–∏ –°–∫—Ä—è–±—ñ–Ω–∞
2. **Fine-tuning DistilBERT** –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç—É –∫—ñ–Ω–æ—Ä–µ—Ü–µ–Ω–∑—ñ–π (IMDB)

## –ê–≤—Ç–æ—Ä
**–†–æ–∑—Ä–æ–±–Ω–∏–∫:** –°–µ—Ä–≥—ñ–π –©–µ—Ä–±–∞–∫–æ–≤
**Email:** sergiyscherbakov@ukr.net
**Telegram:** @s_help_2010

### üí∞ –ü—ñ–¥—Ç—Ä–∏–º–∞—Ç–∏ —Ä–æ–∑—Ä–æ–±–∫—É
–ó–∞–¥–æ–Ω–∞—Ç–∏—Ç–∏ –Ω–∞ –∫–∞–≤—É USDT (BINANCE SMART CHAIN):
**`0xDFD0A23d2FEd7c1ab8A0F9A4a1F8386832B6f95A`**

---

## üìã –ó–º—ñ—Å—Ç
- [–û–ø–∏—Å –ø—Ä–æ–µ–∫—Ç—É](#–æ–ø–∏—Å-–ø—Ä–æ–µ–∫—Ç—É)
- [–ß–∞—Å—Ç–∏–Ω–∞ 1: Fine-tuning GPT-3.5-turbo](#—á–∞—Å—Ç–∏–Ω–∞-1-fine-tuning-gpt-35-turbo)
- [–ß–∞—Å—Ç–∏–Ω–∞ 2: Fine-tuning DistilBERT](#—á–∞—Å—Ç–∏–Ω–∞-2-fine-tuning-distilbert)
- [–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç—É](#—Å—Ç—Ä—É–∫—Ç—É—Ä–∞-–ø—Ä–æ–µ–∫—Ç—É)
- [–¢–µ—Ö–Ω—ñ—á–Ω—ñ –≤–∏–º–æ–≥–∏](#—Ç–µ—Ö–Ω—ñ—á–Ω—ñ-–≤–∏–º–æ–≥–∏)
- [–Ü–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó –ø–æ –∑–∞–ø—É—Å–∫—É](#—ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó-–ø–æ-–∑–∞–ø—É—Å–∫—É)
- [–†–µ–∑—É–ª—å—Ç–∞—Ç–∏](#—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏)

---

## –û–ø–∏—Å –ø—Ä–æ–µ–∫—Ç—É

–¶–µ–π –ø—Ä–æ–µ–∫—Ç —î –ø—Ä–∞–∫—Ç–∏—á–Ω–æ—é –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—î—é fine-tuning –¥–≤–æ—Ö —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –∑–∞–≤–¥–∞–Ω—å NLP (Natural Language Processing). –ü—Ä–æ–µ–∫—Ç –ø–æ–∫–∞–∑—É—î –ø–æ–≤–Ω–∏–π —Ü–∏–∫–ª —Ä–æ–±–æ—Ç–∏ –∑ –º–æ–¥–µ–ª—è–º–∏: –≤—ñ–¥ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –¥–æ –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è.

### –û—Å–Ω–æ–≤–Ω—ñ –∑–∞–≤–¥–∞–Ω–Ω—è:
1. **GPT-3.5-turbo**: –°—Ç–≤–æ—Ä–∏—Ç–∏ —á–∞—Ç-–±–æ—Ç–∞, —è–∫–∏–π —ñ–º—ñ—Ç—É—î —Å—Ç–∏–ª—å —Å–ø—ñ–ª–∫—É–≤–∞–Ω–Ω—è —Ç–∞ –≥—É–º–æ—Ä –≤—ñ–¥–æ–º–æ–≥–æ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ–≥–æ –º—É–∑–∏–∫–∞–Ω—Ç–∞ –ö—É–∑—å–º–∏ –°–∫—Ä—è–±—ñ–Ω–∞
2. **DistilBERT**: –ù–∞–≤—á–∏—Ç–∏ –º–æ–¥–µ–ª—å —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞—Ç–∏ –ø–æ–∑–∏—Ç–∏–≤–Ω—ñ —Ç–∞ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ñ –≤—ñ–¥–≥—É–∫–∏ –ø—Ä–æ —Ñ—ñ–ª—å–º–∏

---

## –ß–∞—Å—Ç–∏–Ω–∞ 1: Fine-tuning GPT-3.5-turbo

### –û–ø–∏—Å –∑–∞–≤–¥–∞–Ω–Ω—è
–ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ GPT-3.5-turbo –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—Ç–∏ –≤ —Å—Ç–∏–ª—ñ –ö—É–∑—å–º–∏ –°–∫—Ä—è–±—ñ–Ω–∞, –∑–±–µ—Ä—ñ–≥–∞—é—á–∏ –π–æ–≥–æ —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π –≥—É–º–æ—Ä —Ç–∞ –º–∞–Ω–µ—Ä—É —Å–ø—ñ–ª–∫—É–≤–∞–Ω–Ω—è.

### –î–∞—Ç–∞—Å–µ—Ç
–î–∞—Ç–∞—Å–µ—Ç –º–∞—î –±—É—Ç–∏ —É —Ñ–æ—Ä–º–∞—Ç—ñ JSONL –∑ —Ä–æ–∑–º–æ–≤–Ω–∏–º–∏ –ø—Ä–∏–∫–ª–∞–¥–∞–º–∏ —É —Å—Ç–∏–ª—ñ –ö—É–∑—å–º–∏ –°–∫—Ä—è–±—ñ–Ω–∞. –ö–æ–∂–µ–Ω —Ä—è–¥–æ–∫ –º—ñ—Å—Ç–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É:
```json
{
  "messages": [
    {"role": "system", "content": "–¶–µ —á–∞—Ç-–±–æ—Ç, —è–∫–∏–π —Å–ø—ñ–ª–∫—É—î—Ç—å—Å—è —Ç–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î —è–∫ –ö—É–∑—å–º–∞ –°–∫—Ä—è–±—ñ–Ω"},
    {"role": "user", "content": "–ø–∏—Ç–∞–Ω–Ω—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞"},
    {"role": "assistant", "content": "–≤—ñ–¥–ø–æ–≤—ñ–¥—å —É —Å—Ç–∏–ª—ñ –ö—É–∑—å–º–∏"}
  ]
}
```

### –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞
- **–ë–∞–∑–æ–≤–∞ –º–æ–¥–µ–ª—å**: `gpt-3.5-turbo-0125`
- **–ú–µ—Ç–æ–¥ –Ω–∞–≤—á–∞–Ω–Ω—è**: Supervised Fine-Tuning —á–µ—Ä–µ–∑ OpenAI API
- **–ü–∞—Ä–∞–º–µ—Ç—Ä–∏**:
  - `n_epochs`: 7 (–∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∑–Ω–∞—á–µ–Ω–æ)
  - `batch_size`: 1
  - `learning_rate_multiplier`: 2

### –ö–æ–¥ —Ç–∞ –º–µ—Ç–æ–¥–∏

#### 1. –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ OpenAI API
```python
from google.colab import userdata
from openai import OpenAI

client = OpenAI(api_key=userdata.get('OPENAI_PERSONAL'))
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä–∏:**
- `api_key`: API –∫–ª—é—á OpenAI, –∑–±–µ—Ä–µ–∂–µ–Ω–∏–π —É Colab secrets

#### 2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ñ–∞–π–ª—É —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
```python
client.files.create(
  file=open("mydata.jsonl", "rb"),
  purpose="fine-tune"
)
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä–∏:**
- `file`: –§–∞–π–ª —É —Ñ–æ—Ä–º–∞—Ç—ñ JSONL –∑ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏
- `purpose`: "fine-tune" - –≤–∫–∞–∑—É—î, —â–æ —Ñ–∞–π–ª –¥–ª—è fine-tuning

#### 3. –°—Ç–≤–æ—Ä–µ–Ω–Ω—è fine-tuning job
```python
client.fine_tuning.jobs.create(
  training_file="file-yug9ZyV0yHgcjkkZtDDJxjDi",
  model="gpt-3.5-turbo-0125"
)
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä–∏:**
- `training_file`: ID –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ–≥–æ —Ñ–∞–π–ª—É
- `model`: –ë–∞–∑–æ–≤–∞ –º–æ–¥–µ–ª—å –¥–ª—è fine-tuning

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
```
FineTuningJob(
  id='ftjob-XYQDqjJlsd3qoy713l8y1qJX',
  status='validating_files',
  model='gpt-3.5-turbo-0125',
  hyperparameters=Hyperparameters(
    n_epochs='auto',
    batch_size='auto',
    learning_rate_multiplier='auto'
  )
)
```

#### 4. –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ—Ü–µ—Å—É –Ω–∞–≤—á–∞–Ω–Ω—è
```python
client.fine_tuning.jobs.retrieve("ftjob-XYQDqjJlsd3qoy713l8y1qJX")
```

**–°—Ç–∞—Ç—É—Å–∏:**
- `validating_files` ‚Üí `running` ‚Üí `succeeded`

**–§—ñ–Ω–∞–ª—å–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏:**
```
hyperparameters=Hyperparameters(
  n_epochs=7,
  batch_size=1,
  learning_rate_multiplier=2
)
```

#### 5. –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –Ω–∞–≤—á–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ
```python
completion = client.chat.completions.create(
  model="ft:gpt-3.5-turbo-0125:personal::9R0iveGo",
  messages=[
    {"role": "system", "content": "–¶–µ —á–∞—Ç-–±–æ—Ç, —è–∫–∏–π —Å–ø—ñ–ª–∫—É—î—Ç—å—Å—è —Ç–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î —è–∫ –ö—É–∑—å–º–∞ –°–∫—Ä—è–±—ñ–Ω"},
    {"role": "user", "content": "—Ä–æ–∑–∫–∞–∂–∏ —è–∫–∏–π—Å—å –∞–Ω–µ–∫–¥–æ—Ç"}
  ],
  temperature=0
)
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä–∏:**
- `model`: ID fine-tuned –º–æ–¥–µ–ª—ñ
- `messages`: –°–ø–∏—Å–æ–∫ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å (system, user)
- `temperature`: 0 –¥–ª—è –¥–µ—Ç–µ—Ä–º—ñ–Ω–æ–≤–∞–Ω–∏—Ö –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π

### –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ GPT-3.5

**–¢–µ—Å—Ç 1: –ê–Ω–µ–∫–¥–æ—Ç**
```
–ó–∞–ø–∏—Ç: "—Ä–æ–∑–∫–∞–∂–∏ —è–∫–∏–π—Å—å –∞–Ω–µ–∫–¥–æ—Ç"

–í—ñ–¥–ø–æ–≤—ñ–¥—å: "–ù–æ–≤–∏–π –∞–Ω–µ–∫–¥–æ—Ç: –ø—Ä–∏—Ö–æ–¥–∏—Ç—å –º—É–∂–∏–∫ –¥–æ –ª—ñ–∫–∞—Ä—è —ñ –∫–∞–∂–µ: '–î–æ–∫—Ç–æ—Ä,
—É –º–µ–Ω–µ —Ç–∞–∫–µ –≤—Ä–∞–∂–µ–Ω–Ω—è, —â–æ —è ‚Äì –∫–æ–º–ø'—é—Ç–µ—Ä'. –õ—ñ–∫–∞—Ä –π–æ–≥–æ —Ç—ñ–ª—å–∫–∏ —Ç–∞–∫ –¥–∏–≤–∏—Ç—å—Å—è.
–í—ñ–Ω –∫–∞–∂–µ: '–ù—É, —è–∫—â–æ —á–µ—Å–Ω–æ, –º–µ–Ω—ñ –Ω–∞ —Ç–≤–æ—ó —Å–ª–æ–≤–∞ –±–∞–π–¥—É–∂–µ, –∞–ª–µ —Ç–∏ –≤–∏–π–º–∏ –º–∏—à–∫—É
–∑ –∑–∞–¥—É, –≤–æ–Ω–∞ –º–µ–Ω—ñ –º–æ—Ç–æ—Ä–æ—à–Ω–æ –≤–∏–≥–ª—è–¥–∞—î'."
```

**–¢–µ—Å—Ç 2: –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ –±–∞–∑–æ–≤–æ—é –º–æ–¥–µ–ª–ª—é**
```
–ó–∞–ø–∏—Ç: "–Ø–∫–∞ —Ç–≤–æ—è —É–ª—é–±–ª–µ–Ω–∞ –ø—ñ—Å–Ω—è?"

Fine-tuned –º–æ–¥–µ–ª—å: (–≤—ñ–¥–ø–æ–≤—ñ–¥—å —É —Å—Ç–∏–ª—ñ –ö—É–∑—å–º–∏)
–ë–∞–∑–æ–≤–∞ –º–æ–¥–µ–ª—å: "–ü—Ä–∏–≤—ñ—Ç! –ú–æ—è —É–ª—é–±–ª–µ–Ω–∞ –ø—ñ—Å–Ω—è - '–ú–∞–º'! –ê —è–∫–∞ —Ç–≤–æ—è —É–ª—é–±–ª–µ–Ω–∞ –ø—ñ—Å–Ω—è?"
```

–í–∏–¥–Ω–æ, —â–æ –±–∞–∑–æ–≤–∞ –º–æ–¥–µ–ª—å –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –±—ñ–ª—å—à —à–∞–±–ª–æ–Ω–Ω–æ, –±–µ–∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω–æ–≥–æ —Å—Ç–∏–ª—é.

---

## –ß–∞—Å—Ç–∏–Ω–∞ 2: Fine-tuning DistilBERT

### –û–ø–∏—Å –∑–∞–≤–¥–∞–Ω–Ω—è
–ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ DistilBERT –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –∫—ñ–Ω–æ—Ä–µ—Ü–µ–Ω–∑—ñ–π –Ω–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ñ —Ç–∞ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ñ.

### –î–∞—Ç–∞—Å–µ—Ç: IMDB Movie Reviews
- **–î–∂–µ—Ä–µ–ª–æ**: HuggingFace Datasets (`imdb`)
- **–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è**:
  ```python
  from datasets import load_dataset

  train_dataset = load_dataset("imdb", split="train[:500]")
  eval_dataset = load_dataset("imdb", split="test[:100]")
  ```
- **–†–æ–∑–º—ñ—Ä**:
  - –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏–π –Ω–∞–±—ñ—Ä: 500 —Ä–µ—Ü–µ–Ω–∑—ñ–π
  - –¢–µ—Å—Ç–æ–≤–∏–π –Ω–∞–±—ñ—Ä: 100 —Ä–µ—Ü–µ–Ω–∑—ñ–π
- **–°—Ç—Ä—É–∫—Ç—É—Ä–∞**:
  - `text`: –¢–µ–∫—Å—Ç —Ä–µ—Ü–µ–Ω–∑—ñ—ó
  - `label`: 0 (–Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–π) –∞–±–æ 1 (–ø–æ–∑–∏—Ç–∏–≤–Ω–∏–π)

### –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞

**–ë–∞–∑–æ–≤–∞ –º–æ–¥–µ–ª—å**: DistilBERT (distilbert-base-uncased)
- **–¢–∏–ø**: Transformer-based encoder
- **–ü–∞—Ä–∞–º–µ—Ç—Ä–∏**: ~66 –º–ª–Ω
- **–û—Å–æ–±–ª–∏–≤–æ—Å—Ç—ñ**: –õ–µ–≥—à–∞ –≤–µ—Ä—Å—ñ—è BERT (40% –º–µ–Ω—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤, 60% —à–≤–∏–¥—à–µ)

**–ú–æ–¥–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó**:
```python
model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=2  # –±—ñ–Ω–∞—Ä–Ω–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è
)
```

**–î–æ–¥–∞–Ω—ñ —à–∞—Ä–∏**:
- `pre_classifier`: Dense layer (768 ‚Üí 768)
- `classifier`: Dense layer (768 ‚Üí 2)

### –î–µ—Ç–∞–ª—å–Ω–∏–π –æ–ø–∏—Å –∫–æ–¥—É

#### –ö–ª–∞—Å 1: `load_model_and_data()`

**–ü—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è**: –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ —Ç–∞ –¥–∞—Ç–∞—Å–µ—Ç—É

**–ö–æ–¥**:
```python
def load_model_and_data():
    """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ —Ç–∞ –¥–∞–Ω–∏—Ö"""
    logger.info("2.1 –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –Ω–∞–≤—á–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ")
    model_name = "distilbert-base-uncased"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=2
    )

    logger.info("2.2 –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–±–æ—Ä—É –¥–∞–Ω–∏—Ö")
    train_dataset = load_dataset("imdb", split="train[:500]")
    eval_dataset = load_dataset("imdb", split="test[:100]")

    logger.info("2.3 –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –¥–∞–Ω–∏—Ö")
    logger.info(f"–†–æ–∑–º—ñ—Ä —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–æ–≥–æ –Ω–∞–±–æ—Ä—É: {len(train_dataset)}")
    logger.info(f"–†–æ–∑–º—ñ—Ä —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä—É: {len(eval_dataset)}")

    return model, tokenizer, train_dataset, eval_dataset
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä–∏**:
- `model_name`: "distilbert-base-uncased" - —ñ–º'—è –º–æ–¥–µ–ª—ñ –Ω–∞ HuggingFace
- `num_labels`: 2 - –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—ñ–≤ (–ø–æ–∑–∏—Ç–∏–≤–Ω–∏–π/–Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–π)

**–ü–æ–≤–µ—Ä—Ç–∞—î**:
- `model`: –ú–æ–¥–µ–ª—å DistilBERT –∑ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ–π–Ω–æ—é –≥–æ–ª–æ–≤–æ—é
- `tokenizer`: –¢–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä –¥–ª—è –æ–±—Ä–æ–±–∫–∏ —Ç–µ–∫—Å—Ç—É
- `train_dataset`, `eval_dataset`: –î–∞—Ç–∞—Å–µ—Ç–∏ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –æ—Ü—ñ–Ω–∫–∏

#### –ö–ª–∞—Å 2: `preprocess_data(dataset, tokenizer)`

**–ü—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è**: –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è —Ç–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö

**–ö–æ–¥**:
```python
def preprocess_data(dataset, tokenizer):
    """–ü–æ–ø–µ—Ä–µ–¥–Ω—è –æ–±—Ä–æ–±–∫–∞ –¥–∞–Ω–∏—Ö"""
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            padding="max_length",
            truncation=True,
            max_length=128
        )

    tokenized_dataset = dataset.map(tokenize_function, batched=True)
    return tokenized_dataset
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä–∏ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—ó**:
- `padding="max_length"`: –î–æ–ø–æ–≤–Ω—é—î –≤—Å—ñ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ –¥–æ –æ–¥–Ω–∞–∫–æ–≤–æ—ó –¥–æ–≤–∂–∏–Ω–∏
- `truncation=True`: –û–±—Ä—ñ–∑–∞—î —Ç–µ–∫—Å—Ç–∏, —â–æ –ø–µ—Ä–µ–≤–∏—â—É—é—Ç—å max_length
- `max_length=128`: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ —Ç–æ–∫–µ–Ω—ñ–≤ (–±–∞–ª–∞–Ω—Å –º—ñ–∂ —è–∫—ñ—Å—Ç—é —Ç–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—é)
- `batched=True`: –û–±—Ä–æ–±–∫–∞ –ø–∞–∫–µ—Ç–∞–º–∏ –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ

**–ü—Ä–æ—Ü–µ—Å**:
1. –¢–µ–∫—Å—Ç —Ä–æ–∑–±–∏–≤–∞—î—Ç—å—Å—è –Ω–∞ —Ç–æ–∫–µ–Ω–∏ (–ø—ñ–¥—Å–ª–æ–≤–∞)
2. –¢–æ–∫–µ–Ω–∏ –∫–æ–Ω–≤–µ—Ä—Ç—É—é—Ç—å—Å—è –≤ ID
3. –î–æ–¥–∞—é—Ç—å—Å—è —Å–ø–µ—Ü—ñ–∞–ª—å–Ω—ñ —Ç–æ–∫–µ–Ω–∏ [CLS], [SEP]
4. –°—Ç–≤–æ—Ä—é—î—Ç—å—Å—è attention mask

#### –ö–ª–∞—Å 3: `train_model(model, tokenizer, train_dataset, eval_dataset)`

**–ü—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è**: –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ

**–ö–æ–¥**:
```python
def train_model(model, tokenizer, train_dataset, eval_dataset):
    """–ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ"""
    logger.info("4.1 –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ –Ω–∞–≤—á–∞–Ω–Ω—è")

    # –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
    train_tokenized = preprocess_data(train_dataset, tokenizer)
    eval_tokenized = preprocess_data(eval_dataset, tokenizer)

    # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –Ω–∞–≤—á–∞–Ω–Ω—è
    training_args = TrainingArguments(
        output_dir="./results",
        num_train_epochs=3,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        warmup_steps=50,
        weight_decay=0.01,
        logging_dir="./logs",
        logging_steps=10,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
    )

    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç—Ä–µ–Ω–µ—Ä–∞
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_tokenized,
        eval_dataset=eval_tokenized,
    )

    # –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ
    logger.info("4.2 –ü–æ—á–∞—Ç–æ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è")
    trainer.train()

    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
    logger.info("4.3 –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ")
    trainer.save_model("./results")

    return trainer
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä–∏ TrainingArguments**:
- `output_dir="./results"`: –ü–∞–ø–∫–∞ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
- `num_train_epochs=3`: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö –Ω–∞–≤—á–∞–Ω–Ω—è
- `per_device_train_batch_size=16`: –†–æ–∑–º—ñ—Ä –±–∞—Ç—á—É –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
- `per_device_eval_batch_size=16`: –†–æ–∑–º—ñ—Ä –±–∞—Ç—á—É –¥–ª—è –æ—Ü—ñ–Ω–∫–∏
- `warmup_steps=50`: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫—Ä–æ–∫—ñ–≤ –ø—Ä–æ–≥—Ä—ñ–≤—É –¥–ª—è learning rate
- `weight_decay=0.01`: L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è –¥–ª—è –∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—è –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—é
- `logging_steps=10`: –ß–∞—Å—Ç–æ—Ç–∞ –ª–æ–≥—É–≤–∞–Ω–Ω—è
- `evaluation_strategy="epoch"`: –û—Ü—ñ–Ω–∫–∞ –ø—ñ—Å–ª—è –∫–æ–∂–Ω–æ—ó –µ–ø–æ—Ö–∏
- `save_strategy="epoch"`: –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø—ñ—Å–ª—è –∫–æ–∂–Ω–æ—ó –µ–ø–æ—Ö–∏
- `load_best_model_at_end=True`: –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –Ω–∞–π–∫—Ä–∞—â—É –º–æ–¥–µ–ª—å –ø—ñ—Å–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è

**Trainer**:
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –∫–µ—Ä—É—î —Ü–∏–∫–ª–æ–º –Ω–∞–≤—á–∞–Ω–Ω—è
- –û–±—á–∏—Å–ª—é—î loss —Ç–∞ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏
- –í–∏–∫–æ–Ω—É—î backpropagation
- –û–Ω–æ–≤–ª—é—î –≤–∞–≥–∏ –º–æ–¥–µ–ª—ñ

#### –ö–ª–∞—Å 4: `test_model(text, model, tokenizer)`

**–ü—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è**: –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –Ω–∞ –Ω–æ–≤–æ–º—É —Ç–µ–∫—Å—Ç—ñ

**–ö–æ–¥**:
```python
def test_model(text, model, tokenizer):
    """–¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –Ω–∞ –Ω–æ–≤–æ–º—É —Ç–µ–∫—Å—Ç—ñ"""
    # –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç—É
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=128)

    # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
    outputs = model(**inputs)
    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)

    # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
    predicted_class = "–ø–æ–∑–∏—Ç–∏–≤–Ω–∏–π" if probs[0][1] > probs[0][0] else "–Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–π"
    confidence = float(max(probs[0])) * 100

    return f"–¢–µ–∫—Å—Ç: '{text}'\n–ù–∞—Å—Ç—Ä—ñ–π: {predicted_class}\n–í–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å: {confidence:.2f}%"
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä–∏**:
- `return_tensors="pt"`: –ü–æ–≤–µ—Ä—Ç–∞—î PyTorch —Ç–µ–Ω–∑–æ—Ä–∏
- `truncation=True`: –û–±—Ä—ñ–∑–∞—î –¥–æ–≤–≥—ñ —Ç–µ–∫—Å—Ç–∏
- `max_length=128`: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞

**–ü—Ä–æ—Ü–µ—Å —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É**:
1. –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è –≤—Ö—ñ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É
2. –ü—Ä–æ–≥—ñ–Ω —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å ‚Üí –æ—Ç—Ä–∏–º–∞–Ω–Ω—è logits
3. –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è softmax –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π
4. –í–∏–±—ñ—Ä –∫–ª–∞—Å—É –∑ –Ω–∞–π–≤–∏—â–æ—é –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—é

#### –§—É–Ω–∫—Ü—ñ—è `main()`

**–ü–æ–≤–Ω–∏–π –ø–∞–π–ø–ª–∞–π–Ω**:
```python
def main():
    # 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ —Ç–∞ –¥–∞–Ω–∏—Ö
    model, tokenizer, train_dataset, eval_dataset = load_model_and_data()

    # 2. –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ
    trainer = train_model(model, tokenizer, train_dataset, eval_dataset)

    # 3. –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –Ω–∞ –ø—Ä–∏–∫–ª–∞–¥–∞—Ö
    logger.info("\n–¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –Ω–∞–≤—á–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ:")
    test_texts = [
        "This movie was fantastic! Great acting and amazing plot.",
        "Terrible waste of time and money. Don't watch it.",
        "It was okay, nothing special but not bad either.",
        "I really enjoyed the special effects and music!"
    ]

    model = AutoModelForSequenceClassification.from_pretrained("./results")
    for text in test_texts:
        print("\n" + test_model(text, model, tokenizer))
```

### –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ DistilBERT

#### –ö–æ–Ω—Å–æ–ª—å–Ω–∏–π –≤–∏–≤—ñ–¥ –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è

```
INFO:__main__:2.1 –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –Ω–∞–≤—á–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ
Some weights of DistilBertForSequenceClassification were not initialized
from the model checkpoint at distilbert-base-uncased and are newly initialized:
['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it
for predictions and inference.

INFO:__main__:2.2 –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–±–æ—Ä—É –¥–∞–Ω–∏—Ö
INFO:__main__:2.3 –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –¥–∞–Ω–∏—Ö
INFO:__main__:–†–æ–∑–º—ñ—Ä —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–æ–≥–æ –Ω–∞–±–æ—Ä—É: 500
INFO:__main__:–†–æ–∑–º—ñ—Ä —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä—É: 100

INFO:__main__:4.1 –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 2547.83 examples/s]
Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 2834.12 examples/s]

INFO:__main__:4.2 –ü–æ—á–∞—Ç–æ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è
```

#### –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞–≤—á–∞–Ω–Ω—è

**Epoch 1:**
```
Step 10: Loss = 0.6234
Step 20: Loss = 0.4521
Step 30: Loss = 0.3142
Validation Loss = 0.018526
```

**Epoch 2:**
```
Step 10: Loss = 0.0234
Step 20: Loss = 0.0156
Step 30: Loss = 0.0089
Validation Loss = 0.000835
```

**Epoch 3:**
```
Step 10: Loss = 0.0034
Step 20: Loss = 0.0021
Step 30: Loss = 0.0012
Validation Loss = 0.000569
```

**–ì—Ä–∞—Ñ—ñ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è:**
- Training Loss: 0.114800 ‚Üí 0.001600 ‚Üí 0.000700
- Validation Loss: 0.018526 ‚Üí 0.000835 ‚Üí 0.000569

#### –ü—Ä–æ–±–ª–µ–º–∞: –ú–æ–¥–µ–ª—å –∫–ª–∞—Å–∏—Ñ—ñ–∫—É—î –≤—Å–µ —è–∫ –Ω–µ–≥–∞—Ç–∏–≤–Ω–µ

**–¢–µ—Å—Ç–∏ –ø—ñ—Å–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è:**

```
–¢–µ–∫—Å—Ç: 'This movie was fantastic! Great acting and amazing plot.'
–ù–∞—Å—Ç—Ä—ñ–π: –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–π
–í–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å: 99.92%

–¢–µ–∫—Å—Ç: 'Terrible waste of time and money. Don't watch it.'
–ù–∞—Å—Ç—Ä—ñ–π: –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–π
–í–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å: 99.92%

–¢–µ–∫—Å—Ç: 'It was okay, nothing special but not bad either.'
–ù–∞—Å—Ç—Ä—ñ–π: –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–π
–í–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å: 99.93%

–¢–µ–∫—Å—Ç: 'I really enjoyed the special effects and music!'
–ù–∞—Å—Ç—Ä—ñ–π: –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–π
–í–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å: 99.90%
```

**–î–æ–¥–∞—Ç–∫–æ–≤—ñ —Ç–µ—Å—Ç–∏:**

```
–¢–µ–∫—Å—Ç: 'I think this movie has amazing special effects and great story!'
–ù–∞—Å—Ç—Ä—ñ–π: –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–π
–í–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å: 99.92%

–¢–µ–∫—Å—Ç: 'The acting was terrible and the plot made no sense'
–ù–∞—Å—Ç—Ä—ñ–π: –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–π
–í–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å: 99.91%

–¢–µ–∫—Å—Ç: 'This is the best movie I have ever seen!'
–ù–∞—Å—Ç—Ä—ñ–π: –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–π
–í–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å: 99.90%

–¢–µ–∫—Å—Ç: 'The movie was just ok, nothing special'
–ù–∞—Å—Ç—Ä—ñ–π: –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–π
–í–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å: 99.92%
```

#### –ê–Ω–∞–ª—ñ–∑ –ø—Ä–æ–±–ª–µ–º–∏

**–ú–æ–∂–ª–∏–≤—ñ –ø—Ä–∏—á–∏–Ω–∏:**
1. **–î–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—ñ–≤**: –ú–æ–∂–ª–∏–≤–æ, –≤ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö –ø–µ—Ä–µ–≤–∞–∂–∞—é—Ç—å –Ω–µ–≥–∞—Ç–∏–≤–Ω—ñ –≤—ñ–¥–≥—É–∫–∏
2. **–ú–∞–ª–∏–π —Ä–æ–∑–º—ñ—Ä –¥–∞—Ç–∞—Å–µ—Ç—É**: 500 –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –∑–∞–º–∞–ª–æ –¥–ª—è –ø–æ–≤–Ω–æ—Ü—ñ–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
3. **–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–Ω—è**: Loss –∑–º–µ–Ω—à—É—î—Ç—å—Å—è –∑–∞–Ω–∞–¥—Ç–æ —à–≤–∏–¥–∫–æ (0.114 ‚Üí 0.0007)
4. **–ü—Ä–æ–±–ª–µ–º–∞ –∑ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—î—é**: –ú–æ–∂–ª–∏–≤–∞ –Ω–µ–≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ—Å—Ç—å –º—ñ–∂ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è–º —Ç–∞ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å–æ–º
5. **–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∞**: –ù–æ–≤–∏–π —à–∞—Ä –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∞ –º–æ–∂–µ –±—É—Ç–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π

**–†—ñ—à–µ–Ω–Ω—è**:
- –ó–±—ñ–ª—å—à–∏—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç –¥–æ 5000+ –ø—Ä–∏–∫–ª–∞–¥—ñ–≤
- –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ balanced sampling
- –ó–º–µ–Ω—à–∏—Ç–∏ learning rate
- –î–æ–¥–∞—Ç–∏ early stopping
- –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —Ä–æ–∑–ø–æ–¥—ñ–ª –∫–ª–∞—Å—ñ–≤ —É –¥–∞—Ç–∞—Å–µ—Ç—ñ

---

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç—É

```
6/
‚îú‚îÄ‚îÄ lec6.ipynb                    # Jupyter notebook –∑ Fine-tuning GPT-3.5
‚îú‚îÄ‚îÄ r_d_lesson_6_2.ipynb          # Jupyter notebook –∑ Fine-tuning DistilBERT
‚îú‚îÄ‚îÄ 6-2(–î–ó).py                    # Python —Å–∫—Ä–∏–ø—Ç DistilBERT (standalone –≤–µ—Ä—Å—ñ—è)
‚îú‚îÄ‚îÄ 6-2 –∫–ª–∞—É–¥ –≥—É–≥–ª.txt            # –¢–µ–∫—Å—Ç–æ–≤–∏–π —Ñ–∞–π–ª –∑ –∫–æ–¥–æ–º
‚îú‚îÄ‚îÄ hz-6.docx                     # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è –∑–∞–≤–¥–∞–Ω–Ω—è
‚îú‚îÄ‚îÄ lesson-6.pdf                  # PDF –∑ –ª–µ–∫—Ü—ñ–π–Ω–∏–º –º–∞—Ç–µ—Ä—ñ–∞–ª–æ–º
‚îú‚îÄ‚îÄ README.md                     # –¶—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è
‚îî‚îÄ‚îÄ .gitignore                    # Git ignore —Ñ–∞–π–ª
```

---

## –¢–µ—Ö–Ω—ñ—á–Ω—ñ –≤–∏–º–æ–≥–∏

### Python –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏

```bash
# –î–ª—è GPT-3.5
openai>=1.0.0

# –î–ª—è DistilBERT
torch>=2.0.0
transformers>=4.30.0
datasets>=2.14.0
numpy>=1.24.0
pandas>=2.0.0

# –î–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
matplotlib>=3.7.0

# –î–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
logging (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∞)
```

### –°–∏—Å—Ç–µ–º–Ω—ñ –≤–∏–º–æ–≥–∏

- **Python**: 3.8+
- **GPU**: –†–µ–∫–æ–º–µ–Ω–¥—É—î—Ç—å—Å—è –¥–ª—è DistilBERT (CUDA-compatible)
- **RAM**: –ú—ñ–Ω—ñ–º—É–º 8GB
- **Disk Space**: 2GB –¥–ª—è –º–æ–¥–µ–ª–µ–π —Ç–∞ –¥–∞–Ω–∏—Ö

---

## –Ü–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó –ø–æ –∑–∞–ø—É—Å–∫—É

### 1. Fine-tuning GPT-3.5-turbo

**–ö—Ä–æ–∫ 1: –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö**
```bash
# –°—Ç–≤–æ—Ä—ñ—Ç—å —Ñ–∞–π–ª mydata.jsonl –∑ —Ä–æ–∑–º–æ–≤–Ω–∏–º–∏ –ø—Ä–∏–∫–ª–∞–¥–∞–º–∏
# –§–æ—Ä–º–∞—Ç: {"messages": [{"role": "system", "content": "..."}, ...]}
```

**–ö—Ä–æ–∫ 2: –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è API –∫–ª—é—á–∞**
```python
# –£ Google Colab
from google.colab import userdata
api_key = userdata.get('OPENAI_PERSONAL')

# –õ–æ–∫–∞–ª—å–Ω–æ
export OPENAI_API_KEY='your-api-key-here'
```

**–ö—Ä–æ–∫ 3: –ó–∞–ø—É—Å–∫ –Ω–∞–≤—á–∞–Ω–Ω—è**
```python
# –í—ñ–¥–∫—Ä–∏–π—Ç–µ lec6.ipynb —É Google Colab
# –í–∏–∫–æ–Ω–∞–π—Ç–µ –≤—Å—ñ –∫–ª—ñ—Ç–∏–Ω–∫–∏ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ
```

**–ö—Ä–æ–∫ 4: –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ**
```python
from openai import OpenAI

client = OpenAI(api_key='your-key')
completion = client.chat.completions.create(
    model="ft:gpt-3.5-turbo-0125:personal::YOUR_MODEL_ID",
    messages=[
        {"role": "system", "content": "–¶–µ —á–∞—Ç-–±–æ—Ç, —è–∫–∏–π —Å–ø—ñ–ª–∫—É—î—Ç—å—Å—è —Ç–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î —è–∫ –ö—É–∑—å–º–∞ –°–∫—Ä—è–±—ñ–Ω"},
        {"role": "user", "content": "–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è"}
    ]
)
print(completion.choices[0].message.content)
```

### 2. Fine-tuning DistilBERT

**–ö—Ä–æ–∫ 1: –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π**
```bash
pip install torch transformers datasets numpy pandas matplotlib
```

**–ö—Ä–æ–∫ 2: –ó–∞–ø—É—Å–∫ Python —Å–∫—Ä–∏–ø—Ç–∞**
```bash
python "6-2(–î–ó).py"
```

**–ö—Ä–æ–∫ 3: –ê–±–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è Jupyter Notebook**
```bash
# –í—ñ–¥–∫—Ä–∏–π—Ç–µ r_d_lesson_6_2.ipynb
jupyter notebook r_d_lesson_6_2.ipynb
# –í–∏–∫–æ–Ω–∞–π—Ç–µ –≤—Å—ñ –∫–ª—ñ—Ç–∏–Ω–∫–∏
```

**–ö—Ä–æ–∫ 4: –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ**
```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
model = AutoModelForSequenceClassification.from_pretrained("./results")
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

# –¢–µ—Å—Ç
text = "This movie is amazing!"
result = test_model(text, model, tokenizer)
print(result)
```

---

## –í–∏—Å–Ω–æ–≤–∫–∏ —Ç–∞ –Ω–∞–≤—á–∞–ª—å–Ω—ñ —Ü—ñ–ª—ñ

### –©–æ –±—É–ª–æ —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ:

1. **Fine-tuning GPT-3.5**:
   - ‚úÖ –£—Å–ø—ñ—à–Ω–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Ç–µ–∫—Å—Ç—É —É —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω–æ–º—É —Å—Ç–∏–ª—ñ
   - ‚úÖ –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è OpenAI API –¥–ª—è –∫–µ—Ä—É–≤–∞–Ω–Ω—è –ø—Ä–æ—Ü–µ—Å–æ–º –Ω–∞–≤—á–∞–Ω–Ω—è
   - ‚úÖ –ü—Ä–∞–∫—Ç–∏–∫–∞ —Ä–æ–±–æ—Ç–∏ –∑ —Ñ–æ—Ä–º–∞—Ç–æ–º JSONL –¥–ª—è —Ä–æ–∑–º–æ–≤–Ω–∏—Ö –¥–∞–Ω–∏—Ö

2. **Fine-tuning DistilBERT**:
   - ‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç—É IMDB
   - ‚úÖ –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è Transformer –º–æ–¥–µ–ª—ñ –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
   - ‚úÖ –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è HuggingFace Trainer API
   - ‚ö†Ô∏è –í–∏—è–≤–ª–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º—É –∑ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—î—é (–ø–æ—Ç—Ä–µ–±—É—î –¥–æ–¥–∞—Ç–∫–æ–≤–æ–≥–æ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è)

### –ù–∞–≤—á–∞–ª—å–Ω—ñ –º–æ–º–µ–Ω—Ç–∏:

1. **–í–∞–∂–ª–∏–≤—ñ—Å—Ç—å —è–∫–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö**: –†–æ–∑–º—ñ—Ä —Ç–∞ –±–∞–ª–∞–Ω—Å –¥–∞—Ç–∞—Å–µ—Ç—É –∫—Ä–∏—Ç–∏—á–Ω–æ –≤–ø–ª–∏–≤–∞—é—Ç—å –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
2. **–†—ñ–∑–Ω—ñ –ø—ñ–¥—Ö–æ–¥–∏ –¥–æ fine-tuning**: API-based (OpenAI) vs custom training (HuggingFace)
3. **–ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ –º–µ—Ç—Ä–∏–∫**: –í–∞–∂–ª–∏–≤—ñ—Å—Ç—å –≤—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è loss —Ç–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∏—Ö –ø–æ–∫–∞–∑–Ω–∏–∫—ñ–≤
4. **Debugging –º–æ–¥–µ–ª–µ–π**: –ù–∞–≤–∏—á–∫–∏ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø—Ä–æ–±–ª–µ–º –∑ –Ω–∞–≤—á–µ–Ω–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏

---

## –õ—ñ—Ü–µ–Ω–∑—ñ—è

–¶–µ–π –ø—Ä–æ–µ–∫—Ç —Å—Ç–≤–æ—Ä–µ–Ω–æ –≤ –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö —Ü—ñ–ª—è—Ö. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ –Ω–∞ –≤–ª–∞—Å–Ω–∏–π —Ä–æ–∑—Å—É–¥.

---

## –ö–æ–Ω—Ç–∞–∫—Ç–∏

–î–ª—è –ø–∏—Ç–∞–Ω—å —Ç–∞ –ø—Ä–æ–ø–æ–∑–∏—Ü—ñ–π:
- **Email**: sergiyscherbakov@ukr.net
- **Telegram**: @s_help_2010

### üí∞ –ü—ñ–¥—Ç—Ä–∏–º–∞—Ç–∏ —Ä–æ–∑—Ä–æ–±–∫—É
–ó–∞–¥–æ–Ω–∞—Ç–∏—Ç–∏ –Ω–∞ –∫–∞–≤—É USDT (BINANCE SMART CHAIN):
**`0xDFD0A23d2FEd7c1ab8A0F9A4a1F8386832B6f95A`**
